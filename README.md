# Syllabus

Date|Content
:---: | :---
July 7 | 1. Read [this blog poster](https://jalammar.github.io/illustrated-word2vec/) about Word2vec<br>2. Read [The Transformer paper](https://arxiv.org/abs/1706.03762), after reading the paper, read [this blog](https://jalammar.github.io/illustrated-transformer/)
July 14 | 1. Read the [ELMo paper](https://aclanthology.org/N18-1202.pdf) and the [BERT paper](https://arxiv.org/abs/1810.04805)<br>2. After reading them, proceed to read blog posts [blog1](https://jalammar.github.io/illustrated-bert/), [blog2](https://jalammar.github.io/illustrated-bert/), and [blog3](https://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/).<br><br>- Examples about Question<br>* How ELMO is different from how people used to train an AI system.<br>* How BERT is different from previous work including ELMO.<br>* What are “common” properties of ELMO and BERT.<br><br>[ Optional ]<br>In a project, we’ve used BERT, RoBERTa, and DistilBERT. Explain what are “key” differences between these three.
July 21 | (Travel)
July 28 | 1. Read the [GPT-2 paper](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)<br>2. After reading it, proceed to read blog posts [blog1](https://jalammar.github.io/illustrated-gpt2/), [blog2](https://www.ibm.com/blog/how-bert-and-gpt-models-change-the-game-for-nlp/), and [blog3](https://huggingface.co/learn/nlp-course/chapter1/4).<br><br>- Examples about Question<br>* The “key” common properties of BERT and GPT-2.<br>* The “key” differences between BERT and GPT-2.<br>* When you would want BERT, and when you would want GPT-2.
August 4 | 1. Review [GPT-3 paper](https://arxiv.org/abs/2005.14165) and a [blog post](http://ai.stanford.edu/blog/in-context-learning/).<br>2. Also, read [this blog post](https://jalammar.github.io/how-gpt3-works-visualizations-animations/) to confirm your understanding.<br>3. [Another blog post](https://www.understandingai.org/p/large-language-models-explained-with)<br><br>- Examples about Question<br>* The “key” differences between GPT-3 and previous work including BERT and GPT-2.<br>* Pros and Cons of this key difference. When would you want one, and when would you want the other?<br>* GPT-2 and GPT-3 are different in how they were used for downstream tasks. But if we only focus on “pre-training”, what are the biggest differences? Hint: They are largely similar in model architecture, and training objective. So the differences lie in different aspects.<br>* Your own answer on “why it works”
August 11 | 1. More papers/readings related to GPT-3 [to-be-added]<br>* https://arxiv.org/pdf/2102.09690.pdf<br>* https://arxiv.org/abs/2101.06804<br>2. Encoder-decoder papers: [BART](https://arxiv.org/abs/1910.13461), [T5](https://arxiv.org/abs/1910.10683)<br>3. Do a paper critic session (1/3)
August 18 | 1. Read [FLAN paper](https://arxiv.org/pdf/2109.01652.pdf), [T0 paper](https://arxiv.org/abs/2110.08207), [Self-instruct paper](https://arxiv.org/abs/2212.10560), and [Alpaca paper](https://crfm.stanford.edu/2023/03/13/alpaca.html).<br>2. Do a paper critic session (2/3)
August 25 | Do a paper critic session (3/3)
September 1 | Wrapping up

# Summary
* [Notion]()

# Reference
* [NLP: Self-supervised Models](https://self-supervised.cs.jhu.edu/sp2023/)
* [Lecture](https://www.youtube.com/watch?v=bZQun8Y4L2A&t=994s)
